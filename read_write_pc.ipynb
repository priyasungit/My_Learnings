{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518a5dcd-99b9-4a6b-add3-8dc8ece4925b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814893c3-0c48-49b6-85e7-da8e997ff6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS wd36schema2\")\n",
    "spark.sql(\n",
    "    \"CREATE VOLUME workspace.wd36schema2.volume1\"\n",
    ")\n",
    "\n",
    "# Now creat\n",
    "# Then create the directory\n",
    "dbutils.fs.mkdirs(\"/Volumes/workspace/wd36schema2/volume1/folder1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19796f5-a168-487d-abb3-bac9e0e55342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists catalog1_dropme;\n",
    "create database if not exists catalog1_dropme.schema1_dropme;\n",
    "create volume if not exists catalog1_dropme.schema1_dropme.volume1_dropme;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb6d7bc-db63-4989-bd41-46e5d193685c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.mkdirs(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme\")\n",
    "#Upload the drive data into this location..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9003c08-a0cf-439a-862a-d7182c36cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/custs\")\n",
    "csv_df2=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\", header=True,inferSchema=True)\n",
    "print(csv_df1.printSchema())\n",
    "\n",
    "print(csv_df2.printSchema())\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "\n",
    "display(csv_df2)#display with produce output in a beautified table format, specific to databricks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de82668-1682-49bb-9dfa-5d3003cd5315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#csv_df1.write.csv(\"/Volumes/workspace/wd36schema2/volume1/folder1/outputdata\")\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "csv_df2=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/custs\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)\n",
    "\n",
    "csv_df2.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dc00f6-eeb9-4063-9724-154356cc78a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df3=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/custs - Copy.csv\",header=True,sep='~').toDF(\"ID~NAME~AGE\")\n",
    "csv_df3.show(2)\n",
    "csv_df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b523f974-0da9-402c-b10e-5aa6507315f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "csv_df2=spark.read.option(\"header\",\"True\").option(\"inferSchema\",\"true\").option(\"sep\",\"~\").format(\"csv\").load(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "csv_df2.show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf66f7c3-1ab9-49f2-8af5-1242877d51b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=[\"dbfs:////Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust*\",\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/txns\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df1.count())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b79572d-c270-4912-88ab-a70c99f36317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str_struct=\"id string,fname string,lname string,age integer,prof string\"\n",
    "csv_df1=spark.read.schema(str_struct).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\",header=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1be8cc-a798-4083-bf4e-546b196716c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\",header=True,inferSchema=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f25938cd-6a49-4115-be17-526a6c30bcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Important part - Using structure type to define custom complex schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c6e2a9-9954-44ef-8aba-1f6ee1b92f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#4000001,Kristina,Chung,55,Pilot\n",
    "#pattern - \n",
    "#import the types library based classes..\n",
    "# define_structure=StructType([StructField(\"colname\",DataType(),True),StructField(\"colname\",DataType(),True)...])\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "custom_schema=StructType([StructField(\"id\",FloatType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType())])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\",header=True,inferSchema=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67529c6b-7bb1-4ae7-aa22-63024762f39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "df1.show(2)\n",
    "display(df1.take/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71c23e4-8e60-42a3-9ca8-b5383ed1db72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfjson1=spark.read.json(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/ebayjson_corrupt.json\")\n",
    "dfjson1.show(2)\n",
    "display(dfjson1.take(22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf73832-b1c5-46b1-a072-f50201f56658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dfxml1=spark.read.xml(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/book.xml\",rowTag=\"book\")\n",
    "dfxml1.show(2)\n",
    "display(dfxml1.take(3))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c7221dd-68e4-4899-9c22-c14bba8ffa18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftable1=spark.read.table(\"workspace.wd36schema.lh_custtbl\")\n",
    "dftable1.show(2)\n",
    "display(dftable1.take(2))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4541847373148649,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "read_write_pc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
