{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518a5dcd-99b9-4a6b-add3-8dc8ece4925b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814893c3-0c48-49b6-85e7-da8e997ff6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS wd36schema2\")\n",
    "spark.sql(\n",
    "    \"CREATE VOLUME workspace.wd36schema2.volume1\"\n",
    ")\n",
    "\n",
    "# Now creat\n",
    "# Then create the directory\n",
    "dbutils.fs.mkdirs(\"/Volumes/workspace/wd36schema2/volume1/folder1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19796f5-a168-487d-abb3-bac9e0e55342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists catalog1_dropme;\n",
    "create database if not exists catalog1_dropme.schema1_dropme;\n",
    "create volume if not exists catalog1_dropme.schema1_dropme.volume1_dropme;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb6d7bc-db63-4989-bd41-46e5d193685c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.mkdirs(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme\")\n",
    "#Upload the drive data into this location..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9003c08-a0cf-439a-862a-d7182c36cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/custs\")\n",
    "csv_df2=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\", header=True,inferSchema=True)\n",
    "print(csv_df1.printSchema())\n",
    "\n",
    "print(csv_df2.printSchema())\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "\n",
    "display(csv_df2)#display with produce output in a beautified table format, specific to databricks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de82668-1682-49bb-9dfa-5d3003cd5315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#csv_df1.write.csv(\"/Volumes/workspace/wd36schema2/volume1/folder1/outputdata\")\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "csv_df2=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/custs\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)\n",
    "\n",
    "csv_df2.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dc00f6-eeb9-4063-9724-154356cc78a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df3=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/custs - Copy.csv\",header=True,sep='~').toDF(\"ID~NAME~AGE\")\n",
    "csv_df3.show(2)\n",
    "csv_df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b523f974-0da9-402c-b10e-5aa6507315f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "csv_df2=spark.read.option(\"header\",\"True\").option(\"inferSchema\",\"true\").option(\"sep\",\"~\").format(\"csv\").load(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "csv_df2.show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf66f7c3-1ab9-49f2-8af5-1242877d51b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=[\"dbfs:////Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust*\",\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/txns\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df1.count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4541847373148649,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "read_write_pc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
