{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4584a443-9077-455d-bdec-416fafeb75e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A catalog named telecom_catalog_assign\n",
    "A schema landing_zone\n",
    "A volume landing_vol\n",
    "Using dbutils.fs.mkdirs, create folders:\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/ /Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/ /Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):\n",
    "a. Volume vs DBFS/FileStore\n",
    "## # b. Why production teams prefer Volumes for regulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0cc858-e640-4cf8-a235-5290659eb12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS telecom_catalog_assign\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS  telecom_catalog_assign.landing_zone\")\n",
    "spark.sql(\n",
    "    \"CREATE VOLUME if not exists telecom_catalog_assign.landing_zone.landing_vol\"\n",
    ")\n",
    "\n",
    "# Now creat\n",
    "# Then create the directory\n",
    "#dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d79745f-0f7e-4626-a9ef-804f70d31855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "# Define the DBFS path for the new file\n",
    "dbfs_path = \"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\" # A common location for user uploads\n",
    "\n",
    "# Use dbutils.fs.put() to write the string content to DBFS\n",
    "# The 'True' argument means 'overwrite' (optional, set to True to replace existing file)\n",
    "dbutils.fs.put(dbfs_path, customer_csv.strip(), True)\n",
    "\n",
    "print(f\"Data successfully written to {dbfs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ce6f6c-6d38-48ca-b328-08d47895b47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_tsv ='''\n",
    "customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "dbfs_path = \"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\"\n",
    "dbutils.fs.put(dbfs_path, usage_tsv.strip(), True)\n",
    "\n",
    "print(f\"Data successfully written to {dbfs_path}\")\n",
    "\n",
    "tower_logs_region1 = '''\n",
    "event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5005|101|TWR01|-80|2025-12-10 10:41:54\n",
    "'''\n",
    "dbfs_path = \"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\"\n",
    "dbutils.fs.put(dbfs_path, tower_logs_region1.strip(), True)\n",
    "\n",
    "print(f\"Data successfully written to {dbfs_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f08364-e914-4715-b416-3a1466946f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb8436e6-84fd-47e6-9e6b-8c1c52fbc58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_logs_region2 = '''\n",
    "event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "dbfs_path = \"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region2.csv\"\n",
    "dbutils.fs.put(dbfs_path, tower_logs_region2.strip(), True)\n",
    "\n",
    "print(f\"Data successfully written to {dbfs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5322ad-6fc6-48ff-8c58-04f03a551348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup\n",
    "\n",
    "Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c648adf3-99d4-47e9-9ed6-bb0522af074d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_multiple_sources=spark.read.csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\",\"dbfs:////Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region2.csv\"],inferSchema=True,header=True,sep='|',pathGlobFilter=\"*.csv\",recursiveFileLookup=True)\n",
    "#.toDF(\"cid\",\"fn\",\"ln\",\"a\",\"p\")\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e27f40-9a8c-4c56-9deb-5a45b19e80aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")'''\n",
    "df_multiple_sources=spark.read.csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\",\"dbfs:////Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region2.csv\"],inferSchema=True,header=True,sep='|',recursiveFileLookup=True)\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc7ad0b-0482-43ee-8a06-a4c5cf1154be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_df1=spark.read.csv(path=\"dbfs:////Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region*.csv\",inferSchema=True,header=True,sep='|')\n",
    "print(csv_df1.count())\n",
    "csv_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43812701-3c58-4de9-84f8-8bd1cbb7a75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers.csv\",inferSchema=False,header=False,sep=',').toDF(\"id\",\"name\",\"age\",\"loc\",\"plan\")\n",
    "df2=csv_df1.where(\"age!='abc' \")\n",
    "csv_df1.printSchema()\n",
    "print(csv_df1.count())\n",
    "csv_df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0ca274-f3d8-4763-9134-6549892d9b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",inferSchema=True,header=True,sep='\\t')\n",
    "print(csv_df1.count())\n",
    "csv_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7495fadd-07c8-4e11-a416-c47c0ada841a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_recursive = spark.read.format(\"csv\") .option(\"header\", \"true\") .option(\"delimiter\", \"|\") .option(\"recursiveFileLookup\", \"true\") .load(\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "\n",
    "print(\"--- DataFrame using recursiveFileLookup='true' ---\")\n",
    "df_recursive.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b527c6-5da0-4b84-9957-01fd7697ed60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_glob = spark.read.format(\"csv\") .option(\"header\", \"true\")  .option(\"delimiter\", \"|\") .option(\"pathGlobFilter\", \"*region*.csv\") .load(\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "\n",
    "print(\"--- DataFrame using pathGlobFilter ('region*.csv') ---\")\n",
    "df_glob.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db64b6d9-4e99-470e-9666-cb19d8d34f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Apply column names using string using toDF function for customer data\n",
    "Apply column names and datatype using the schema function for usage data\n",
    "Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cffc810-5238-41f6-a599-b86e13a7422a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#customer_id|voice_mins|data_mb|sms_count\n",
    "str_struct=\"customer_id integer,voice_mins integer,data_mb integer,sms_count integer\"\n",
    "csv_df1=spark.read.schema(str_struct).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,sep='\\t')\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show()\n",
    "\n",
    "\n",
    "#2. Important part - Using structure type to define custom complex schema.\n",
    "#4000001,Kristina,Chung,55,Pilot\n",
    "#pattern - \n",
    "#import the types library based classes..\n",
    "# define_structure=StructType([StructField(\"colname\",DataType(),True),StructField(\"colname\",DataType(),True)...])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5455794-b506-4cd4-9300-0b39f32d4626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#event_id|customer_id|tower_id|signal_strength|          timestamp\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"event_id\",IntegerType(),False),StructField(\"customer_id\",IntegerType(),False),StructField(\"tower_id\",StringType(),False),StructField(\"signal_strength\",IntegerType(),False),StructField(\"timestamp\",StringType(),False)])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",pathGlobFilter=\"*.csv\",header='True',sep='|')\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dcbb393-d57d-4803-a6fc-158b2f9b231c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "Write customer data into CSV format using overwrite mode\n",
    "Write usage data into CSV format using append mode\n",
    "Write tower data into CSV format with header enabled and custom separator (|)\n",
    "Read the tower data in a dataframe and show only 5 rows.\n",
    "Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d655452-0607-4fb4-b2d7-c767275b7999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"loc\", StringType(), True),\n",
    "    StructField(\"plan\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers.csv\",\n",
    "    schema=schema,\n",
    "    header=False,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "df2 = csv_df1.where(\"age != 'abc'\")\n",
    "\n",
    "display(csv_df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e578a33c-2c77-44b0-ab95-13ddb81b7fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\",\n",
    "    inferSchema=True, \n",
    "    header=False, \n",
    "    sep=','\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d54d887-1f8a-49ee-8043-4171714f4e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Now you can safely overwrite\n",
    "csv_df1.write.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customercsv\",\n",
    "    sep=',',\n",
    "    header=False,\n",
    "    mode='overwrite',\n",
    "    compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba76c0d-ec72-4599-9f7d-95946e2802a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",inferSchema=True,header=True,sep='\\t')\n",
    "\n",
    "csv_df1.write.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsv\",\n",
    "    sep=',',\n",
    "    header=True,\n",
    "    mode='append',\n",
    "    compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e8ab3c-af2f-4088-b372-8d8744c83995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write tower data into CSV format with header enabled and custom separator (|)\n",
    "Read the tower data in a dataframe and show only 5 rows.\n",
    "Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7475c28f-1654-41ea-a76a-6533cf00193f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#event_id|customer_id|tower_id|signal_strength|          timestamp\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"event_id\",IntegerType(),False),StructField(\"customer_id\",IntegerType(),False),StructField(\"tower_id\",StringType(),False),StructField(\"signal_strength\",IntegerType(),False),StructField(\"timestamp\",StringType(),False)])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",pathGlobFilter=\"*.csv\",header='True',sep='|')\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show()\n",
    "\n",
    "csv_df1.write.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towercsv\",\n",
    "    sep='|',\n",
    "    header=True,\n",
    "    mode='append',\n",
    "    compression='gzip')\n",
    "\n",
    "display(csv_df1.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ac598a2-eabc-4269-a664-1807613c1b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "Write customer data into JSON format using overwrite mode\n",
    "Write usage data into JSON format using append mode and snappy compression format\n",
    "Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "Read the tower data in a dataframe and show only 5 rows.\n",
    "Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c2024e-7108-47ef-82b9-091a58970d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\",\n",
    "    inferSchema=True, \n",
    "    header=False, \n",
    "    sep=','\n",
    ")\n",
    "\n",
    "csv_df1.write.json(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersjson\",\n",
    "    mode='overwrite',\n",
    "    compression='gzip')\n",
    "\n",
    "csv_df1.write.parquet(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersparquet\",\n",
    "    mode='overwrite',\n",
    "    compression='gzip')\n",
    "\n",
    "csv_df1.write.orc(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersorc\",\n",
    "    mode='overwrite',\n",
    "    compression='zlib')\n",
    "    \n",
    "csv_df1.write .format(\"delta\") .mode(\"overwrite\") .save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersdelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdda98ba-d049-41b3-b04e-69eb19010b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",inferSchema=True,header=True,sep='\\t')\n",
    "csv_df1.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsvjson\",mode='append',compression='snappy')\n",
    "csv_df1.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsvparquet\",mode='append',compression='snappy')\n",
    "csv_df1.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsvorc\",mode='append',compression='snappy')\n",
    "\n",
    "csv_df1.write.format(\"delta\") .mode(\"append\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsdelta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b236cedd-a416-428e-9fdf-6802dd1d69b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"event_id\",IntegerType(),False),StructField(\"customer_id\",IntegerType(),False),StructField(\"tower_id\",StringType(),False),StructField(\"signal_strength\",IntegerType(),False),StructField(\"timestamp\",StringType(),False)])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",pathGlobFilter=\"*.csv\",header='True',sep='|')\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show()\n",
    "csv_df1.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towercsvjson\",mode='ignore',compression='snappy')\n",
    "csv_df1.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towercsvparquet\",mode='ignore',compression='snappy')\n",
    "display(csv_df1)\n",
    "csv_df1.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towercsvorc\",mode='ignore',compression='snappy')\n",
    "\n",
    "csv_df1.write.format(\"delta\") .mode(\"ignore\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towercsvdelta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cce3cc6-acb3-4456-8f43-4756624cd317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "Write usage data into Parquet format using error mode\n",
    "Write tower data into Parquet format with gzip compression option\n",
    "Read the usage data in a dataframe and show only 5 rows.\n",
    "Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b1cac2-f13f-4fa6-854e-d3870f84e65e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\",\n",
    "    inferSchema=True, \n",
    "    header=False, \n",
    "    sep=','\n",
    ")\n",
    "csv_df1.write.parquet(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersparquet\",\n",
    "    mode='overwrite',\n",
    "    compression='gzip')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ad172f-20bc-4610-8889-020e4d3d02c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",inferSchema=True,header=True,sep='\\t')\n",
    "\n",
    "csv_df1.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsvparquet_cp\",mode='error',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e043bbb8-9ae4-4dea-92b2-4086c9a47faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"event_id\",IntegerType(),False),StructField(\"customer_id\",IntegerType(),False),StructField(\"tower_id\",StringType(),False),StructField(\"signal_strength\",IntegerType(),False),StructField(\"timestamp\",StringType(),False)])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",pathGlobFilter=\"*.csv\",header='True',sep='|')\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show()\n",
    "csv_df1.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/towercsvparquet\",mode='ignore',compression='gzip')\n",
    "display(csv_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df0f957-d6f6-4bea-85eb-d5df25b45f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "par_df1=spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagetsvparquet_cp\")\n",
    "par_df1.show(5)\n",
    "display(par_df1.take(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e080f7d0-c20d-4a5b-bc8e-40d774afa446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write customer data using saveAsTable() as a managed table\n",
    "Write usage data using saveAsTable() with overwrite mode\n",
    "Drop the managed table and verify data removal\n",
    "Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "Use spark.read.sql to write some simple queries on the above tables created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62907bb0-5613-448c-b168-c243d32bbbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\",\n",
    "    inferSchema=True, \n",
    "    header=False, \n",
    "    sep=','\n",
    ")\n",
    "csv_df1.write.saveAsTable(\"workspace.wd36schema2.lh_custtbl1\",mode='overwrite')\n",
    "\n",
    "spark.sql(\"select * from workspace.wd36schema2.lh_custtbl1\").show()\n",
    "display(spark.sql(\"show create table workspace.wd36schema2.lh_custtbl1\"))\n",
    "#csv_df1.write.droptable(\"workspace.wd36schema2.lh_custtbl1\",True)\n",
    "#spark.sql(\"drop table if exists workspace.wd36schema2.lh_custtbl1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cf38fb-694e-4e8b-931e-9a9d9baf1a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",inferSchema=True,header=True,sep='\\t')\n",
    "\n",
    "csv_df1.write.saveAsTable(\"workspace.wd36schema2.lh_usagetbl1\",mode='overwrite')\n",
    "spark.sql(\"select * from workspace.wd36schema2.lh_usagetbl1\").show()\n",
    "spark.sql(\"create table if not exists workspace.wd36schema2.lh_usagetbl2 as select * from workspace.wd36schema2.lh_usagetbl1 where 1=0\")\n",
    "spark.sql(\"select * from workspace.wd36schema2.lh_usagetbl2\").show()\n",
    "display(spark.sql(\"show create table workspace.wd36schema2.lh_usagetbl2\"))\n",
    "csv_df1.write.insertInto(\"workspace.wd36schema2.lh_usagetbl2\",overwrite=True)\n",
    "spark.sql(\"select * from workspace.wd36schema2.lh_usagetbl2\").show()\n",
    "display(spark.sql(\"show create table workspace.wd36schema2.lh_usagetbl2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c20a77-d147-42c4-b0a0-456f54b4959f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write customer data using insertInto() in a new table and find the behavior\n",
    "Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359e4a3f-aab8-41e4-8643-657a73df2411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\",\n",
    "    inferSchema=True, \n",
    "    header=False, \n",
    "    sep=','\n",
    ")\n",
    "csv_df1.write.saveAsTable(\"workspace.wd36schema2.lh_custtbl1\",mode='overwrite')\n",
    "\n",
    "spark.sql(\"select * from workspace.wd36schema2.lh_custtbl1\").show()\n",
    "display(spark.sql(\"show create table workspace.wd36schema2.lh_custtbl1\"))\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS workspace.wd36schema2.lh_custtbl2\n",
    "    AS SELECT * FROM workspace.wd36schema2.lh_custtbl1 WHERE 1=0\n",
    "\"\"\")\n",
    "display(spark.sql(\"select * from workspace.wd36schema2.lh_custtbl2\" ))\n",
    "csv_df1.write.insertInto(\"workspace.wd36schema2.lh_custtbl2\",overwrite=True)\n",
    "display(spark.sql(\"select * from workspace.wd36schema2.lh_custtbl2\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870f37c4-c0ad-40cf-8827-b46a473dc42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Write customer data into XML format using rowTag as cust\n",
    "Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ce7017-c49a-4579-872f-64020e045e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\n",
    "    path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers_2.csv\",\n",
    "    inferSchema=True, \n",
    "    header=False, \n",
    "    sep=','\n",
    ")\n",
    "csv_df1.write.xml(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/xmlout\",mode=\"ignore\",rowTag=\"cust\")\n",
    "csv_df2=spark.read.format(\"xml\").option(\"rowTag\",\"cust\").load(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/xmlout\")\n",
    "csv_df2.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc05d1ac-8598-4d1e-bbf0-59e8e8ac16d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",inferSchema=True,header=True,sep='\\t')\n",
    "csv_df1.write.xml(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/usagexmlout\",mode=\"overwrite\",rowTag=\"usage\")\n",
    "csv_df2=spark.read.format(\"xml\").option(\"rowTag\",\"usage\").load(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/usagexmlout\")\n",
    "csv_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93078113-2b06-4e06-b29a-764268180907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_orc_cust_df=spark.read.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersorc\")\n",
    "read_orc_cust_df.take(2)\n",
    "write_cust_orc_pqt=read_orc_cust_df.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersorcparquet\",mode=\"overwrite\")\n",
    "csv_df2=spark.read.format(\"parquet\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customersorcparquet\")\n",
    "#display(csv_df2.show(5))\n",
    "\n",
    "#csv_df2.collect()\n",
    "# Removed .show() on write_cust_orc_pqt, as write.parquet() returns None\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_usecase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
