{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a48ba68-cff9-4e0c-b0c0-e4bea2b1a360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logistics_df=spark.read.csv(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/logistics_source1\",inferSchema=True,header=True).toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\")\n",
    "logistics_df.show(logistics_df.count(),truncate=False)\n",
    "logistics_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfcc5b5-50cc-4891-a154-a978f6bdacff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_cnt=logistics_df.count()\n",
    "print(\"Total count of records in the dataframe is {}\".format(total_cnt))\n",
    "distinct_records=logistics_df.select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\").distinct().count()\n",
    "print(\"Distinct count of records in the dataframe is {}\".format(distinct_records))\n",
    "print(\"de-duplicated record (all columns) count\",logistics_df.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given shipment_id column count\",logistics_df.dropDuplicates(['shipment_id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(logistics_df.describe())\n",
    "display(logistics_df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212ec0a7-1a43-4eed-9ba6-6aa74c5ca590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a. Passive Data Munging - (File: logistics_source1 and logistics_source2)\n",
    "Without modifying the data, identify:\n",
    "Shipment IDs that appear in both master_v1 and master_v2\n",
    "Records where:\n",
    "\n",
    "shipment_id is non-numeric\n",
    "age is not an integer\n",
    "Count rows having: 3. fewer columns than expected 4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32eea089-e905-482e-bef0-097d9a5f1d07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "master_v1=spark.read.csv(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/logistics_source1\",inferSchema=True,header=True).toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\")\n",
    "master_v2=spark.read.csv(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/logistics_source2\",inferSchema=True,header=True).toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\")\n",
    "master_v2.printSchema()\n",
    "##Master id appear common in both the dataframes\n",
    "master_id=master_v1.join(master_v2,[\"shipment_id\"],\"inner\").select(master_v1[\"shipment_id\"], master_v1[\"first_name\"], master_v1[\"last_name\"], master_v1[\"age\"], master_v1[\"role\"])\n",
    "master_id.show(master_id.count(),truncate=False)\n",
    "###combining both the DS\n",
    "master_df=master_v1.unionByName(master_v2,allowMissingColumns=True) \n",
    "master_df.show(master_df.count(),truncate=False)\n",
    "display(master_df)\n",
    "master_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ab9069-0ba5-4ba9-b8b6-15a85ee8e03f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "non_numeric_S_ids = master_df.filter(master_df['shipment_id'].rlike(\"[^0-9]\"))\n",
    "\n",
    "print(f\"Found {non_numeric_S_ids.count()} records with non-numeric shipment IDs.\")\n",
    "non_numeric_S_ids.show(truncate=False)\n",
    "\n",
    "#####Age is not an integer#############################3\n",
    "##M1\n",
    "non_numeric_age = master_df.filter(master_df['age'].rlike(\"[^0-9]\"))\n",
    "\n",
    "print(f\"Found {non_numeric_age.count()} records with non-integer shipment IDs.\")\n",
    "non_numeric_age.show(truncate=False)\n",
    "##M2\n",
    "\n",
    "\n",
    "non_integer_age = master_df.filter(\n",
    "    ~F.col(\"age\").rlike(\"^\\\\d+$\") & F.col(\"age\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Found {non_integer_age.count()} records where age is not a clean integer string.\")\n",
    "non_integer_age.show(truncate=False)\n",
    "\n",
    "##Count rows having: 3. fewer columns than expected 4. more columns than expected\n",
    "missing_columns_df=master_df.na.drop( how='any',subset=[\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\"]).count()\n",
    "print(f\"Found {missing_columns_df} records with missing columns.\")\n",
    "\n",
    "extra_columns_df=master_df.na.drop( how='any',subset=[\"hub_location\",\"vehicle_type\"]).count()\n",
    "print(f\"Found {extra_columns_df} records with extra columns.\")\n",
    "display(extra_columns_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31ebd43-862a-4b45-bb14-396690d5597e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read both files without enforcing schema\n",
    "Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276f13d6-afbb-4ad2-b26e-6e58d57200f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "master_v1_df=spark.read.csv(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/logistics_source1\",header=True)\n",
    "add_ds_mv1=master_v1_df.select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\").withColumn(\"datasource\",lit(\"system1\"))\n",
    "display(add_ds_mv1)\n",
    "master_v2_df=spark.read.csv(\"/Volumes/workspace/wd36schema2/ingestion_volume/target/logistics_source2\",header=True)\n",
    "add_ds_mv2=master_v2_df.selectExpr(\"*\", \"'system2' as datasource\").toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"datasource\")\n",
    "display(add_ds_mv2)\n",
    "master_dfe=add_ds_mv1.unionByName(add_ds_mv2,allowMissingColumns=True)\n",
    "master_dfe.show(master_dfe.count(),truncate=False)\n",
    "master_dfe.printSchema()\n",
    "select_cols= master_dfe.select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"datasource\").toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"datasource\")\n",
    "display(select_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9bd0a13-e171-40cc-b06c-b82782836988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cleansing (removal of unwanted datasets)\n",
    "\n",
    "Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "Join Readiness Rule - Drop records where the join key is null: shipment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7bffbd-dff2-4746-ba5a-62f93510e368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "man_column_check=select_cols.na.drop(how='any',subset=[\"shipment_id\",\"role\"])\n",
    "man_column_check.show(man_column_check.count(),truncate=False)\n",
    "compl_rule=man_column_check.na.drop(how=\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "display(compl_rule)\n",
    "join_readiness_rule=compl_rule.na.drop(subset=[\"shipment_id\"])\n",
    "display(join_readiness_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8767bbd-a624-4c75-9cc8-a3fd0ca96d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Scrubbing (convert raw to tidy)\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "6. Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1290ca-d9f2-453e-89af-de8844af5e7d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"hub_location\":90},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768696810209}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "age_def_rule=join_readiness_rule.na.fill(\"-1\",subset=[\"age\"])\n",
    "display(age_def_rule)\n",
    "vel_typ_rule=age_def_rule.na.fill(\"unknown\",subset=[\"vehicle_type\"])\n",
    "display(vel_typ_rule)\n",
    "find_replace_values_dict={'ten':'-1','':'-1'}\n",
    "scrubbeddf2=vel_typ_rule.na.replace(find_replace_values_dict,subset=[\"age\"])\n",
    "display(scrubbeddf2)\n",
    "#Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "find_replace_values_dict2={'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "scrubbeddf3=scrubbeddf2.na.replace(find_replace_values_dict2,subset=[\"vehicle_type\"])\n",
    "display(scrubbeddf3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7407f299-e67b-462b-b569-10e988b21355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format\n",
    "Creating shipments Details data Dataframe creation\n",
    "\n",
    "Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "As this data is a clean json data, it doesn't require any cleansing or scrubbing.\n",
    "Standardizations:\n",
    "\n",
    "Add a column\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    ": domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "Column Uniformity: role - Convert to lowercase\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "vehicle_type - Convert values to UPPERCASE\n",
    "Source Files: DF of logistics_shipment_detail_3000.json hub_location - Convert values to initcap case\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)\n",
    "Format Standardization:\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "Convert shipment_date to yyyy-MM-dd\n",
    "Ensure shipment_cost has 2 decimal precision\n",
    "Data Type Standardization\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "age: Cast String to Integer\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "shipment_weight_kg: Cast to Double\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "is_expedited: Cast to Boolean\n",
    "Naming Standardization\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "Rename: first_name to staff_first_name\n",
    "Rename: last_name to staff_last_name\n",
    "Rename: hub_location to origin_hub_city\n",
    "Reordering columns logically in a better standard format:\n",
    "Source File: DF of Data from all 3 files\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "Deduplication:\n",
    "\n",
    "Apply Record Level De-Duplication\n",
    "Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff3ba29-cb5c-4013-a6ae-0e7b7e5772e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation\n",
    "\n",
    "Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6e3ec0-7fd7-480e-bfee-5c4c15087c46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json( \"/Volumes/workspace/wd36schema2/ingestion_volume/logistics_shipment_detail_3000.json\" )\n",
    "\n",
    "df.show(df.count(),truncate=False)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d146fb3-b431-4d6a-9709-713aed237020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add a column\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    ": domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a637925-6ea3-4020-949a-9da0230d6999",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 15"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "add_col1_df = df.withColumns({\"domain\": lit(\"logistics\"),\n",
    "    \"ingestion_timestamp\":current_timestamp(), \"is_expedited\": lit(\"False\")})\n",
    "add_col1_df.show(add_col1_df.count(), truncate=False)\n",
    "display(add_col1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f958f156-605b-47ac-a973-ac488deb3c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Column Uniformity: role - Convert to lowercase\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "vehicle_type - Convert values to UPPERCASE\n",
    "Source Files: DF of logistics_shipment_detail_3000.json hub_location - Convert values to initcap case\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683ab0e5-3718-4bcd-a1f8-be85751291f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 18"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, upper, initcap\n",
    "scrubbeddf3_lw=scrubbeddf3.withColumns({\"shipment_id\":lower(col(\"shipment_id\")),\"first_name\":lower(col(\"first_name\")),\"last_name\":lower(col(\"last_name\")),\"age\":lower(col(\"age\")),\"role\":lower(col(\"role\")),\"hub_location\":initcap(col(\"hub_location\")),\"vehicle_type\": upper(col (\"vehicle_type\")),\"datasource\": lower(col(\"datasource\"))})\n",
    "display(scrubbeddf3_lw)\n",
    "\n",
    "case_conv_json_df=add_col1_df.withColumns({\"cargo_type\":lower(col(\"cargo_type\")),\"destination_city\":lower(col(\"destination_city\")),\"order_id\":lower(col(\"order_id\")),  \"payment_mode\":lower(col(\"payment_mode\")),\n",
    "\"shipment_cost\":lower(col(\"shipment_cost\")),\n",
    "\"shipment_date\":lower(col(\"shipment_date\")),\n",
    "\"shipment_id\":lower(col(\"shipment_id\")),\n",
    "\"shipment_status\":lower(col(\"shipment_status\")),\n",
    "\"shipment_weight_kg\":lower(col(\"shipment_weight_kg\")),\n",
    "\"source_city\":lower(col(\"source_city\")),\n",
    "\"vehicle_type\":lower(col(\"vehicle_type\")),\n",
    "\"domain\":lower(col(\"domain\")),\n",
    " \"ingestion_timestamp\":lower(col(\"ingestion_timestamp\")),\n",
    "  \"is_expedited\":lower(col(\"is_expedited\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d8c970-12b7-48e6-9c6f-38fb7ea65bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88dca398-75da-498d-967b-068676d2e082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603615df-9f0b-47dc-ba32-08fd574d0cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d02b97-a4a3-4273-bd60-e3478b84eb24",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768772418601}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Cell 17"
    }
   },
   "outputs": [],
   "source": [
    "age_cast = scrubbeddf3_lw.withColumn(\"age\", scrubbeddf3_lw[\"age\"].cast(\"int\"))\n",
    "display(age_cast)\n",
    "rename = age_cast.withColumnsRenamed({\n",
    "    \"first_name\": \"staff_first_name\",\n",
    "    \"last_name\": \"staff_last_name\",\n",
    "    \"hub_location\": \"origin_hub_city\"\n",
    "})\n",
    "display(rename)\n",
    "from pyspark.sql.functions import col\n",
    "shipment_weight_cast = case_conv_json_df.withColumns({\n",
    "    \"shipment_weight_kg\": case_conv_json_df[\"shipment_weight_kg\"].cast(\"Double\"),\n",
    "    \"is_expedited\": case_conv_json_df[\"is_expedited\"].cast(\"Boolean\")\n",
    "})\n",
    "shipment_weight_cast.printSchema()\n",
    "display(shipment_weight_cast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7780c9a-6bb9-4fb8-b43c-1c25b9b107d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format Standardization:\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "Convert shipment_date to yyyy-MM-dd\n",
    "Ensure shipment_cost has 2 decimal precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953bcca0-ff78-454e-b65f-deaae9b39eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,round,col,date_format\n",
    "formatteddf=shipment_weight_cast.withColumns({\n",
    "    \"shipment_date\": date_format(to_date(col(\"shipment_date\"), \"dd-MM-yy\"), \"yyyy-MM-dd\"),\n",
    "    \"shipment_cost\": round(col(\"shipment_cost\"),2)})\n",
    "formatteddf.printSchema()\n",
    "display(formatteddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3564253-f50b-4d53-9f2f-fb4b30c1eb60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Type Standardization\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "age: Cast String to Integer\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "shipment_weight_kg: Cast to Double\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "is_expedited: Cast to Boolean\n",
    "Naming Standardization\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "Rename: first_name to staff_first_name\n",
    "Rename: last_name to staff_last_name\n",
    "Rename: hub_location to origin_hub_city\n",
    "Reordering columns logically in a better standard format:\n",
    "Source File: DF of Data from all 3 files\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0814708d-ed58-4cbf-85c1-da58f50fefbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reordering columns logically in a better standard format: Source File: DF of Data from all 3 files shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "joined_df=rename.unionByName(shipment_weight_cast,allowMissingColumns=True)\n",
    "display(joined_df)\n",
    "reorder_joined_df=joined_df.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"shipment_cost\",\"ingestion_timestamp\")\n",
    "display(reorder_joined_df)\n",
    "\n",
    "\n",
    "#Reordering columns logically in a better standard format: Source File: DF of Data from all 3 files shipment_id (Identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c648d6-70c8-4e65-a32a-cb7712a94836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "\n",
    "Apply Record Level De-Duplication\n",
    "Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3661c1-49b8-4d2e-83c7-b4bf7103c9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_lvl_dedup=reorder_joined_df.distinct()\n",
    "display(row_lvl_dedup)\n",
    "col_lvl_dedup=row_lvl_dedup.dropDuplicates(subset=['shipment_id'])\n",
    "display(col_lvl_dedup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f059664-9383-4ec6-970b-350fe4f54696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed\n",
    "\n",
    "Adding of Columns (Data Enrichment)\n",
    "Creating new derived attributes to enhance traceability and analytical capability.\n",
    "\n",
    "1. Add Audit Timestamp (load_dt) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "Action: Add a column load_dt using the function current_timestamp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ba3093-1975-4b44-b345-664d95f128ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, concat_ws\n",
    "add_tmsp_df=rename.withColumn(\"load_dt\",current_timestamp())\n",
    "display(add_tmsp_df)\n",
    "ad_full_n_column=add_tmsp_df.select(col(\"shipment_id\").alias(\"shipment_id\"),concat_ws(\" \",col(\"staff_first_name\"),col(\"staff_last_name\")).alias(\"Full_name\"),\"role\",\"origin_hub_city\",\"vehicle_type\",\"datasource\",\"load_dt\")\n",
    "display(ad_full_n_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cba137-9348-4ead-8fe0-9b78a858ed1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define Route Segment (route_segment) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    "Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "Action: Combine source_city and destination_city with a hyphen.\n",
    "Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738f831f-80f2-428b-8da9-ff30f6f18e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID. Action: Combine vehicle_type and shipment_id to create a composite key. Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce16393-6a08-4a72-900a-1f9f43805dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771cb80a-1807-488c-8274-5e1b45f4bda1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768767397194}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit\n",
    "stand_src_df=formatteddf.select(col(\"shipment_id\"),\"order_id\",concat(col(\"source_city\"),lit(\"-\"),col(\"destination_city\")).alias(\"route_segment\"),\"cargo_type\",\"payment_mode\",\"shipment_status\",\"shipment_cost\",\"shipment_weight_kg\",\"shipment_date\",\"vehicle_type\",concat(col(\"vehicle_type\"),lit(\"_\"),col(\"shipment_id\")).alias(\"vehicle_identifier\"),\"domain\",\n",
    "\"ingestion_timestamp\",\"is_expedited\")\n",
    "display(stand_src_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db0994c-a733-451c-b097-27ff74e3043e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deriving of Columns (Time Intelligence)\n",
    "Extracting temporal features from dates to enable period-based analysis and reporting.\n",
    "Source File: logistics_shipment_detail_3000.json\n",
    "1. Derive Shipment Year (shipment_year)\n",
    "\n",
    "Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    "Action: Extract the year component from shipment_date.\n",
    "Result: \"2024-04-23\" -> 2024\n",
    "2. Derive Shipment Month (shipment_month)\n",
    "\n",
    "Scenario: Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "Action: Extract the month component from shipment_date.\n",
    "Result: \"2024-04-23\" -> 4 (April)\n",
    "3. Flag Weekend Operations (is_weekend)\n",
    "\n",
    "Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\n",
    "4. Flag shipment status (is_expedited)\n",
    "\n",
    "Scenario: The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "Action: Flag as 'True' if the shipment_status IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5a28cb-20e4-4f98-93e4-46ca9cb4cfd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Derive Shipment Year (shipment_year)\n",
    "Scenario: Management needs an annual performance report to compare growth year-over-year. Action: Extract the year component from shipment_date. Result: \"2024-04-23\" -> 2024 2. Derive Shipment Month (shipment_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd82785-e2ea-4c87-ab75-04d4b7ee7065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, month,year\n",
    "shipment_year_df=stand_src_df.withColumn(\"shipment_year\",year(col(\"shipment_date\").cast(\"date\")))\n",
    "display(shipment_year_df)\n",
    "shipment_month_df=shipment_year_df.withColumn(\"shipment_month\",month(col(\"shipment_date\").cast(\"date\")))\n",
    "display(shipment_month_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76894458-bcf0-41c5-9a23-b26b2a0ec7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Flag Weekend Operations (is_weekend)\n",
    "\n",
    "Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\n",
    "4. Flag shipment status (is_expedited)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e8f806-bb3a-4e48-a218-aeec5f3f5a7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Geo Enrichment Join"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c7e270-1d6f-4076-97aa-34a71f2b388b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix ImportError: len from pyspark.sql.functions"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf451f89-bd4c-4711-a294-1e150eeae18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek, when\n",
    "Weekend_flag =shipment_month_df.withColumn( \"is_weekend\", when(dayofweek(col(\"shipment_date\").cast(\"date\")).isin(1, 7), True).otherwise(False) ) \n",
    "display(Weekend_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ee0fae-4d2b-4750-b8b7-addadf9e6684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Flag shipment status (is_expedited)\n",
    "Scenario: The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "Action: Flag as 'True' if the shipment_status IN_TRANSIT or DELIVERED.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e3b8b7-b321-4bd1-b213-6c1669dfbb98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60b658b-836e-4364-9400-15e7b72c2eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "shipment_flag_df = Weekend_flag.withColumn(\n",
    "    \"is_expedited\",\n",
    "    when(col(\"shipment_status\").isin(\"IN_TRANSIT\", \"DELIVERED\"), True).otherwise(False)\n",
    ")\n",
    "display(shipment_flag_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a959fab8-9a73-4fda-a0cc-076e381dc641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, datediff, current_date, try_divide\n",
    "df_calc_cost = shipment_flag_df.withColumn( \"cost_per_kg\", try_divide(col(\"shipment_cost\"), col(\"shipment_weight_kg\")) )\n",
    "df_calc_date_diff = df_calc_cost.withColumn( \"days_since_shipment\", datediff(current_date(), col(\"shipment_date\").cast(\"date\")) )\n",
    "df_calc = df_calc_date_diff.withColumn( \"tax_amount\", col(\"shipment_cost\") * 0.18 )\n",
    "display(df_calc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64220a3d-b7b3-43de-bdb2-d4491d1b6d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Remove/Eliminate (drop, select, selectExpr)\n",
    "Excluding unnecessary or redundant columns to optimize storage and privacy.\n",
    "Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "1. Remove Redundant Name Columns\n",
    "\n",
    "Scenario: Since we have already created the full_name column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "Action: Drop the first_name and last_name columns.\n",
    "Logic: df.drop(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e290f9-ba43-4efb-9e2a-f963ee27242b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Full_name\":90},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768783314556}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ad_full_n_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c93c2f-3f8c-43be-acfb-d6b33877b41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Splitting & Merging/Melting of Columns\n",
    "Reshaping columns to extract hidden values or combine fields for better analysis.\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "1. Splitting (Extraction) Breaking one column into multiple to isolate key information.\n",
    "\n",
    "Split Order Code:\n",
    "Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "order_prefix (\"ORD\")\n",
    "order_sequence (\"100000\")\n",
    "Split Date:\n",
    "Action: Split shipment_date into three separate columns for partitioning:\n",
    "ship_year (2024)\n",
    "ship_month (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90c2c27-03a6-42ed-85e9-a49bd80c360f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768854714714}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring,length,col,day\n",
    "#display(df_calc)\n",
    "split_df1=df_calc.withColumn(\"order_prefix\",substring(col(\"order_id\"),0,3))\n",
    "split_df2=df_calc.withColumn(\"order_sequence\",substring(col(\"order_id\"),4,length(col(\"order_id\"))))\n",
    "#display(split_df2)\n",
    "split_df3=split_df2.withColumn(\"shipment_day\",day(col(\"shipment_date\")))\n",
    "display(split_df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932401ae-94d5-4d75-bb70-81b163947291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "display(ad_full_n_column)\n",
    "3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "UDF1: Complex Incentive Calculation\n",
    "Scenario: The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "Action: Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "\n",
    "Logic:\n",
    "\n",
    "IF Role == 'Driver' AND Age > 50:\n",
    "Bonus = 15% of Salary (Reward for Seniority)\n",
    "IF Role == 'Driver' AND Age < 30:\n",
    "Bonus = 5% of Salary (Encouragement for Juniors)\n",
    "ELSE:\n",
    "Bonus = 0\n",
    "Result: A new derived column projected_bonus is generated for every row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65ec452-26d3-4a30-a9aa-e42223e4a280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bonus_calc(role, age, sr_salary,jr_salary): \n",
    "    if role == \"driver\" and age > 50:     \n",
    "        return sr_salary * 0.15\n",
    "    elif role == \"driver\" and age< 30:     \n",
    "        return jr_salary * 0.05 \n",
    "    else:\n",
    "         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293a47e5-e773-42a1-b186-c4ef9376d40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "bonus_udf = udf(bonus_calc, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9e8614-6f90-4e85-81be-2ac3c1739283",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768786003322}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_bonus = rename.withColumn(\n",
    "    \"projected_bonus\",\n",
    "    bonus_udf(col(\"role\"), col(\"age\"), lit(20000),lit(10000))\n",
    ")\n",
    "display(df_with_bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af33584-8fc9-45e9-b9f4-1609a43f12f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7347b8aa-31fb-4016-b874-e4ffc83d3beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def mask_identity(name):\n",
    "    if name is None or len(name) < 3:\n",
    "        return name\n",
    "    return name[:2] + \"****\" + name[-1]\n",
    "\n",
    "mask_udf = udf(mask_identity, StringType())\n",
    "\n",
    "df_masked = df_with_bonus.withColumn(\n",
    "    \"masked_name\",\n",
    "    mask_udf(col(\"staff_first_name\"))\n",
    ")\n",
    "display(df_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6320476-2e0e-4f18-be65-1b0ee3e68b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Core Curation & Processing (Pre-Wrangling)\n",
    "Applying business logic to focus, filter, and summarize data before final analysis.\n",
    "\n",
    "1. Select (Projection)\n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: The Driver App team only needs location data, not sensitive HR info.\n",
    "Action: Select only first_name, role, and hub_location.\n",
    "2. Filter (Selection)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: We need a report on active operational problems.\n",
    "Action: Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "Scenario: Insurance audit for senior staff.\n",
    "Action: Filter rows where age > 50.\n",
    "3. Derive Flags & Columns (Business Logic)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: Identify high-value shipments for security tracking.\n",
    "Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "Scenario: Flag weekend operations for overtime calculation.\n",
    "Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "4. Format (Standardization)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: Finance requires readable currency formats.\n",
    "Action: Format shipment_cost to string like \"30,695.80\".\n",
    "Scenario: Standardize city names for reporting.\n",
    "Action: Format source_city to Uppercase (e.g., \"chennai\"  \"CHENNAI\").\n",
    "5. Group & Aggregate (Summarization)\n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: Regional staffing analysis.\n",
    "Action: Group by hub_location and Count the number of staff.\n",
    "Scenario: Fleet capacity analysis.\n",
    "Action: Group by vehicle_type and Sum the shipment_weight_kg.\n",
    "6. Sorting (Ordering)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: Prioritize the most expensive shipments.\n",
    "Action: Sort by shipment_cost in Descending order.\n",
    "Scenario: Organize daily dispatch schedule.\n",
    "Action: Sort by shipment_date (Ascending).\n",
    "7. Limit (Top-N Analysis)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: Dashboard snapshot of critical delays.\n",
    "Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c38cf4-8699-448d-a7b7-a494c20f5bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location_df=df_masked.select(\"staff_first_name\",\"role\",\"origin_hub_city\")\n",
    "insurance_df=df_masked.select(\"*\").where(col(\"age\")>50)\n",
    "display(location_df)\n",
    "display(insurance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0fac5c-9889-43f7-981e-13891b2e120a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Filter (Selection)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: We need a report on active operational problems.\n",
    "Action: Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "Scenario: Insurance audit for senior staff.\n",
    "Action: Filter rows where age > 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2274c40d-30ec-42c7-8167-207fe310cfb6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Order Join and Sequence Number"
    }
   },
   "outputs": [],
   "source": [
    "filter_df1=split_df3.select(\"*\").where((col(\"shipment_status\")==\"delayed\") | (col(\"shipment_status\")==\"returned\"))\n",
    "display(filter_df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21059e4-5bf4-4878-bec6-f3e6302efa36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Derive Flags & Columns (Business Logic)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: Identify high-value shipments for security tracking.\n",
    "Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "Scenario: Flag weekend operations for overtime calculation.\n",
    "Action: Create flag is_weekend = True if day is Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8473e2-1373-48f8-bd35-8f6a9289d5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "derive_col=filter_df1.withColumn(\"is_high_value\",when(col(\"shipment_cost\")>50000,True).otherwise(False))\n",
    "display(derive_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2b380d-280a-4111-a615-ee701d67ba60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Scenario: Finance requires readable currency formats.\n",
    "Action: Format shipment_cost to string like \"30,695.80\".\n",
    "Scenario: Standardize city names for reporting.\n",
    "Action: Format source_city to Uppercase (e.g., \"chennai\"  \"CHENNAI\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2dd778-860c-4c38-b82e-919de7a238f3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768788219741}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number, concat, lit, col \n",
    "df_currency = derive_col.withColumn( \"shipment_cost_fmt\", concat( lit(\"\"), format_number(col(\"shipment_cost\"), 2) ) )\n",
    "display(df_currency)\n",
    "sort_shipment_cost = df_currency.orderBy(col(\"shipment_cost\").desc())\n",
    "display(sort_shipment_cost)\n",
    "daily_dispatch = sort_shipment_cost.orderBy(col(\"shipment_date\").asc())\n",
    "display(daily_dispatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8406d2-a4b6-4bfa-bc1d-6259bc934682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: Dashboard snapshot of critical delays.\n",
    "#Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows.\n",
    "dashboard_df=daily_dispatch.select(\"*\").where(col(\"shipment_status\")==\"delayed\").orderBy(col(\"shipment_cost\").desc()).limit(10)\n",
    "display(dashboard_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45dcf2c1-5d4e-404f-9da2-be7f7a16773f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Group & Aggregate (Summarization)\n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: Regional staffing analysis.\n",
    "Action: Group by hub_location and Count the number of staff.\n",
    "Scenario: Fleet capacity analysis.\n",
    "Action: Group by vehicle_type and Sum the shipment_weight_kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f408930-3c72-44fe-866e-f2fa57929c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "Reg_analysis=df_masked.groupBy(\"origin_hub_city\").agg(countDistinct(\"shipment_id\").alias(\"Total staff\"))\n",
    "display(Reg_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459e0ae3-ba0e-4fcc-b934-0f8bdee8d6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Join Operation\n",
    "staff_df=df_masked.select(\"*\")\n",
    "shipment_df=daily_dispatch.select(\"*\")\n",
    "# Join Operation \n",
    "inner_join_df = ( staff_df .join(shipment_df, on=\"shipment_id\", how=\"inner\") ) \n",
    "display(inner_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c009ead-7ad6-47db-b345-141ef0f7b18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "left_join_df = ( staff_df .join(shipment_df, on=\"shipment_id\", how=\"left\") ) .where(shipment_df.shipment_id.isNull())\n",
    "display(left_join_df)\n",
    "right_join_df = ( staff_df .join(shipment_df, on=\"shipment_id\", how=\"right\") )\n",
    "display(right_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53ee42d-b5ec-4ba7-a3fa-f11eb7a5facc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768791695064}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "staff_a = staff_df.alias(\"a\")\n",
    "staff_b = staff_df.alias(\"b\")\n",
    "\n",
    "peer_pairs = (\n",
    "    staff_a.join(\n",
    "        staff_b,\n",
    "        (col(\"a.origin_hub_city\") == col(\"b.origin_hub_city\")) &\n",
    "        (col(\"a.shipment_id\") != col(\"b.shipment_id\")),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"a.shipment_id\").alias(\"staff_id_A\"),\n",
    "        col(\"b.shipment_id\").alias(\"staff_id_B\"),\n",
    "        col(\"a.origin_hub_city\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(peer_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d5a0d2-d524-41d0-aa99-20a7497fce25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Full Outer Join (Reconciliation):\n",
    "#Scenario: A complete audit to find both idle drivers AND unassigned shipments in one view.\n",
    "#Action: Perform a Full Outer Join on shipment_id.\n",
    "full_outer_join_df = ( staff_df .join(shipment_df, on=\"shipment_id\", how=\"full\") )\n",
    "display(full_outer_join_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25755aee-7e00-48f3-af24-e21631fe7da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_df=staff_df.select(\"*\").where(col(\"role\")==\"driver\")\n",
    "shipment_df=dashboard_df.join(driver_df)\n",
    "display(shipment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92874aa0-cac6-4600-923a-36c0599cd7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Left Semi Join (Existence Check):\n",
    "Scenario: \"Show me the details of Drivers who have at least one shipment.\" (Standard filtering).\n",
    "Action: staff_df.join(shipments_df, \"shipment_id\", \"left_semi\").\n",
    "Benefit: Performance optimization; it stops scanning the right table once a match is found.\n",
    "Left Anti Join (Negation Check):\n",
    "Scenario: \"Show me the details of Drivers who have never touched a shipment.\"\n",
    "Action: staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03dbc855-e80f-464e-a859-687a6c07fdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "semi_lft_df = ( driver_df .join(shipment_df, on=\"shipment_id\", how=\"left_semi\") )\n",
    "display(semi_lft_df)\n",
    "anti_lft_df = ( staff_df .join(shipment_df, on=\"shipment_id\", how=\"left_anti\") )\n",
    "display(anti_lft_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb1ec05-5d45-4b42-b0fe-b8674e36255d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv\n",
    "\n",
    "Scenario: Validation. Check if the hub_location in the staff file exists in the dataframe of corporate Master_City_List.csv.\n",
    "Action: Compare values against this Master_City_List list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e80763f-c3a9-4df3-a3a7-9148283358c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Master_city_df=spark.read.csv(\"/Volumes/workspace/wd36schema2/ingestion_volume/Master_City_List.csv\",header=True,inferSchema=True)\n",
    "Master_city_df_rn=Master_city_df.withColumnRenamed(\"city_name\",\"origin_hub_city\")\n",
    "display(Master_city_df_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26913d97-f78d-406c-a44e-e6436c583436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "semi_drvr_df=(staff_df.join(Master_city_df_rn,on=\"origin_hub_city\",how=\"left_semi\"))\n",
    "display(semi_drvr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5210fde9-159c-46cb-9f90-4cd261d77a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s=staff_df.alias(\"s\")\n",
    "m=Master_city_df_rn.alias(\"m\")\n",
    "geo_enriched_df = ( s.join( m, on=\"origin_hub_city\", how=\"left\" ) .select(col( \"s.*\"), col(\"m.Latitude\").alias(\"Lat\"), col(\"m.Longitude\").alias(\"Long\" ) ))\n",
    "display(geo_enriched_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8f4b8c-d827-4230-9ed7-389965233d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Scenario: Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "Action: Flatten the Star Schema. Join Staff, Shipments, and Vehicle_Master into one wide table (wide_shipment_history) so analysts don't have to perform joins during reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdabbbb-f54d-4af2-a970-f5fb4a1ad4e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix ambiguous reference error for vehicle_type in Vehicle_df creation"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0ba001-d425-4478-b9fa-dcaf47651fd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix ambiguous column reference in join/select"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit\n",
    "Vehicle_df = staff_df.withColumn(\"vehicle_identifier\", concat(col(\"vehicle_type\"), lit(\"_\"), col(\"shipment_id\")))\n",
    "display(Vehicle_df)\n",
    "\n",
    "# Alias both DataFrames to avoid ambiguity\n",
    "a = staff_df.alias(\"a\")\n",
    "b = shipment_df.alias(\"b\")\n",
    "denormalized_tbl = a.join(b, on=\"shipment_id\", how=\"full\").select(col(\"a.*\"), col(\"b.age\").alias(\"shipment_age\"))\n",
    "display(denormalized_tbl)\n",
    "\n",
    "denormalized_tbl.write.saveAsTable(\"workspace.wd36schema2.wide_shipment_history\", mode=\"overwrite\")\n",
    "display(spark.sql(\"select * from workspace.wd36schema2.wide_shipment_history\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb37d42e-f26e-44ec-8c09-1c0390aa6c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Source Files:\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)\n",
    "\n",
    "Scenario: \"Who are the Top 3 Drivers by Cost in each Hub?\"\n",
    "Action:\n",
    "Partition by hub_location.\n",
    "Order by total_shipment_cost Descending.\n",
    "Apply dense_rank() and `row_number()\n",
    "Filter where rank or row_number <= 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecb9a45-8929-402f-969b-d14cec207409",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768868401675}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, desc, dense_rank, substring, length, col, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "a = staff_df.alias(\"a\")\n",
    "b = shipment_df.alias(\"b\")\n",
    "orderjoineddf = (\n",
    "    a.join(b, on=\"origin_hub_city\", how=\"inner\")\n",
    "     .select(\n",
    "         col(\"a.*\"),\n",
    "         col(\"b.shipment_cost_fmt\"),\n",
    "         col(\"b.shipment_date\"),\n",
    "         regexp_replace(substring(col(\"b.shipment_cost_fmt\"), 2, length(col(\"b.shipment_cost_fmt\"))), \",\", \"\").cast(\"double\").alias(\"shipment_cost\"),\n",
    "     )\n",
    ")\n",
    "display(orderjoineddf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sk_orderjoinedf = orderjoineddf.withColumn(\n",
    "    \"seqnum\",\n",
    "    row_number().over(Window.partitionBy(\"origin_hub_city\").orderBy(desc(\"shipment_cost_fmt\")))\n",
    ")\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "Rk_orderjoinedf = orderjoineddf.withColumn(\n",
    "    \"Rank\",\n",
    "    dense_rank().over(Window.partitionBy(\"origin_hub_city\").orderBy(desc(\"shipment_cost_fmt\")))\n",
    ")\n",
    "display(Rk_orderjoinedf)\n",
    "\n",
    "first_3_seq_num = sk_orderjoinedf.filter(sk_orderjoinedf.seqnum <= 3)\n",
    "display(first_3_seq_num)\n",
    "\n",
    "first_3_rank = Rk_orderjoinedf.filter(Rk_orderjoinedf.Rank <= 3)\n",
    "display(first_3_rank)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1264e188-1c8d-4c95-bd46-60db3f966d12",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768875090337}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead,lag,asc\n",
    "sk_orderjoinedf1=orderjoineddf.withColumn(\"nexttransamt\",lead(\"shipment_cost_fmt\",1,-1).over(Window.partitionBy(\"shipment_id\").orderBy(asc(\"shipment_date\")))).withColumn(\"priortransamt\",lag(\"shipment_cost_fmt\",1,-1).over(Window.partitionBy(\"shipment_id\").orderBy(asc(\"shipment_date\"))))\n",
    "display(sk_orderjoinedf1)\n",
    "\n",
    "pattern_df = sk_orderjoinedf1.withColumn(\n",
    "    \"transpattern\",\n",
    "    when((col(\"priortransamt\") == -1), \"first transaction\").\n",
    "    when(col(\"shipment_cost_fmt\") > col(\"priortransamt\"), \"increase\").\n",
    "    when(col(\"shipment_cost_fmt\") < col(\"priortransamt\"), \"decrease\"). # This line was cut off\n",
    "    otherwise(\"no change\") # Added a default condition for when amt == priortransamt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0a5527-bd4c-4c99-9ba6-2e38e8776c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a=master_v1.select(\"shipment_id\")\n",
    "b=master_v2.select(\"shipment_id\")\n",
    "intersectdf=a.intersect(b)\n",
    "print(\"intersection\",intersectdf.count())\n",
    "intersectdf.show(100)\n",
    "\n",
    "subtractdf=b.subtract(a)\n",
    "print(\"df1 subtraction df2\",subtractdf.count())\n",
    "subtractdf.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc9b73b-75fe-44a9-bad9-f3994b25e94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Cast shipment_cost_fmt to double, using try_cast logic\n",
    "\n",
    "\n",
    "rollupdf = orderjoineddf.rollup(\"origin_hub_city\", \"vehicle_type\")  .agg(F.sum(\"shipment_cost\").alias(\"sumamt\"), F.avg(\"shipment_cost\").alias(\"avgamt\")) .orderBy(\"origin_hub_city\", \"sumamt\")\n",
    "display(rollupdf)\n",
    "cubedf=orderjoineddf.cube(\"origin_hub_city\",\"vehicle_type\").agg(F.sum(\"shipment_cost\").alias(\"sumamt\"),F.avg(\"shipment_cost\").alias(\"avgamt\")).orderBy(\"origin_hub_city\",\"sumamt\")\n",
    "display(cubedf)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_USECase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
